Using backend: pytorch
2025-06-13 11:28:14,411 - INFO - config: configs/default_gres.yaml
2025-06-13 11:28:14,416 - INFO - set random seed: 1999
/nfs/data_todi/jli/softwares/local/anaconda3/envs/ipdn_alessio/lib/python3.8/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2025-06-13 11:28:22,893 - INFO - Parameters: 156.74M
2025-06-13 11:28:22,895 - INFO - Load pretrain from backbones/sp_unet_backbone.pth
2025-06-13 11:28:23,046 - WARNING - The model and loaded state dict do not match exactly

size mismatch for criterion.loss_weight: copying a param with shape torch.Size([3]) from checkpoint, the shape in current model is torch.Size([8]).
unexpected key in source state_dict: lang_proj.weight, lang_proj.bias, stm.x_mask.0.weight, stm.x_mask.0.bias, stm.x_mask.2.weight, stm.x_mask.2.bias

missing keys in source state_dict: text_encoder.embeddings.word_embeddings.weight, text_encoder.embeddings.position_embeddings.weight, text_encoder.embeddings.token_type_embeddings.weight, text_encoder.embeddings.LayerNorm.weight, text_encoder.embeddings.LayerNorm.bias, text_encoder.encoder.layer.0.attention.self.query.weight, text_encoder.encoder.layer.0.attention.self.query.bias, text_encoder.encoder.layer.0.attention.self.key.weight, text_encoder.encoder.layer.0.attention.self.key.bias, text_encoder.encoder.layer.0.attention.self.value.weight, text_encoder.encoder.layer.0.attention.self.value.bias, text_encoder.encoder.layer.0.attention.output.dense.weight, text_encoder.encoder.layer.0.attention.output.dense.bias, text_encoder.encoder.layer.0.attention.output.LayerNorm.weight, text_encoder.encoder.layer.0.attention.output.LayerNorm.bias, text_encoder.encoder.layer.0.intermediate.dense.weight, text_encoder.encoder.layer.0.intermediate.dense.bias, text_encoder.encoder.layer.0.output.dense.weight, text_encoder.encoder.layer.0.output.dense.bias, text_encoder.encoder.layer.0.output.LayerNorm.weight, text_encoder.encoder.layer.0.output.LayerNorm.bias, text_encoder.encoder.layer.1.attention.self.query.weight, text_encoder.encoder.layer.1.attention.self.query.bias, text_encoder.encoder.layer.1.attention.self.key.weight, text_encoder.encoder.layer.1.attention.self.key.bias, text_encoder.encoder.layer.1.attention.self.value.weight, text_encoder.encoder.layer.1.attention.self.value.bias, text_encoder.encoder.layer.1.attention.output.dense.weight, text_encoder.encoder.layer.1.attention.output.dense.bias, text_encoder.encoder.layer.1.attention.output.LayerNorm.weight, text_encoder.encoder.layer.1.attention.output.LayerNorm.bias, text_encoder.encoder.layer.1.intermediate.dense.weight, text_encoder.encoder.layer.1.intermediate.dense.bias, text_encoder.encoder.layer.1.output.dense.weight, text_encoder.encoder.layer.1.output.dense.bias, text_encoder.encoder.layer.1.output.LayerNorm.weight, text_encoder.encoder.layer.1.output.LayerNorm.bias, text_encoder.encoder.layer.2.attention.self.query.weight, text_encoder.encoder.layer.2.attention.self.query.bias, text_encoder.encoder.layer.2.attention.self.key.weight, text_encoder.encoder.layer.2.attention.self.key.bias, text_encoder.encoder.layer.2.attention.self.value.weight, text_encoder.encoder.layer.2.attention.self.value.bias, text_encoder.encoder.layer.2.attention.output.dense.weight, text_encoder.encoder.layer.2.attention.output.dense.bias, text_encoder.encoder.layer.2.attention.output.LayerNorm.weight, text_encoder.encoder.layer.2.attention.output.LayerNorm.bias, text_encoder.encoder.layer.2.intermediate.dense.weight, text_encoder.encoder.layer.2.intermediate.dense.bias, text_encoder.encoder.layer.2.output.dense.weight, text_encoder.encoder.layer.2.output.dense.bias, text_encoder.encoder.layer.2.output.LayerNorm.weight, text_encoder.encoder.layer.2.output.LayerNorm.bias, text_encoder.encoder.layer.3.attention.self.query.weight, text_encoder.encoder.layer.3.attention.self.query.bias, text_encoder.encoder.layer.3.attention.self.key.weight, text_encoder.encoder.layer.3.attention.self.key.bias, text_encoder.encoder.layer.3.attention.self.value.weight, text_encoder.encoder.layer.3.attention.self.value.bias, text_encoder.encoder.layer.3.attention.output.dense.weight, text_encoder.encoder.layer.3.attention.output.dense.bias, text_encoder.encoder.layer.3.attention.output.LayerNorm.weight, text_encoder.encoder.layer.3.attention.output.LayerNorm.bias, text_encoder.encoder.layer.3.intermediate.dense.weight, text_encoder.encoder.layer.3.intermediate.dense.bias, text_encoder.encoder.layer.3.output.dense.weight, text_encoder.encoder.layer.3.output.dense.bias, text_encoder.encoder.layer.3.output.LayerNorm.weight, text_encoder.encoder.layer.3.output.LayerNorm.bias, text_encoder.encoder.layer.4.attention.self.query.weight, text_encoder.encoder.layer.4.attention.self.query.bias, text_encoder.encoder.layer.4.attention.self.key.weight, text_encoder.encoder.layer.4.attention.self.key.bias, text_encoder.encoder.layer.4.attention.self.value.weight, text_encoder.encoder.layer.4.attention.self.value.bias, text_encoder.encoder.layer.4.attention.output.dense.weight, text_encoder.encoder.layer.4.attention.output.dense.bias, text_encoder.encoder.layer.4.attention.output.LayerNorm.weight, text_encoder.encoder.layer.4.attention.output.LayerNorm.bias, text_encoder.encoder.layer.4.intermediate.dense.weight, text_encoder.encoder.layer.4.intermediate.dense.bias, text_encoder.encoder.layer.4.output.dense.weight, text_encoder.encoder.layer.4.output.dense.bias, text_encoder.encoder.layer.4.output.LayerNorm.weight, text_encoder.encoder.layer.4.output.LayerNorm.bias, text_encoder.encoder.layer.5.attention.self.query.weight, text_encoder.encoder.layer.5.attention.self.query.bias, text_encoder.encoder.layer.5.attention.self.key.weight, text_encoder.encoder.layer.5.attention.self.key.bias, text_encoder.encoder.layer.5.attention.self.value.weight, text_encoder.encoder.layer.5.attention.self.value.bias, text_encoder.encoder.layer.5.attention.output.dense.weight, text_encoder.encoder.layer.5.attention.output.dense.bias, text_encoder.encoder.layer.5.attention.output.LayerNorm.weight, text_encoder.encoder.layer.5.attention.output.LayerNorm.bias, text_encoder.encoder.layer.5.intermediate.dense.weight, text_encoder.encoder.layer.5.intermediate.dense.bias, text_encoder.encoder.layer.5.output.dense.weight, text_encoder.encoder.layer.5.output.dense.bias, text_encoder.encoder.layer.5.output.LayerNorm.weight, text_encoder.encoder.layer.5.output.LayerNorm.bias, text_encoder.encoder.layer.6.attention.self.query.weight, text_encoder.encoder.layer.6.attention.self.query.bias, text_encoder.encoder.layer.6.attention.self.key.weight, text_encoder.encoder.layer.6.attention.self.key.bias, text_encoder.encoder.layer.6.attention.self.value.weight, text_encoder.encoder.layer.6.attention.self.value.bias, text_encoder.encoder.layer.6.attention.output.dense.weight, text_encoder.encoder.layer.6.attention.output.dense.bias, text_encoder.encoder.layer.6.attention.output.LayerNorm.weight, text_encoder.encoder.layer.6.attention.output.LayerNorm.bias, text_encoder.encoder.layer.6.intermediate.dense.weight, text_encoder.encoder.layer.6.intermediate.dense.bias, text_encoder.encoder.layer.6.output.dense.weight, text_encoder.encoder.layer.6.output.dense.bias, text_encoder.encoder.layer.6.output.LayerNorm.weight, text_encoder.encoder.layer.6.output.LayerNorm.bias, text_encoder.encoder.layer.7.attention.self.query.weight, text_encoder.encoder.layer.7.attention.self.query.bias, text_encoder.encoder.layer.7.attention.self.key.weight, text_encoder.encoder.layer.7.attention.self.key.bias, text_encoder.encoder.layer.7.attention.self.value.weight, text_encoder.encoder.layer.7.attention.self.value.bias, text_encoder.encoder.layer.7.attention.output.dense.weight, text_encoder.encoder.layer.7.attention.output.dense.bias, text_encoder.encoder.layer.7.attention.output.LayerNorm.weight, text_encoder.encoder.layer.7.attention.output.LayerNorm.bias, text_encoder.encoder.layer.7.intermediate.dense.weight, text_encoder.encoder.layer.7.intermediate.dense.bias, text_encoder.encoder.layer.7.output.dense.weight, text_encoder.encoder.layer.7.output.dense.bias, text_encoder.encoder.layer.7.output.LayerNorm.weight, text_encoder.encoder.layer.7.output.LayerNorm.bias, text_encoder.encoder.layer.8.attention.self.query.weight, text_encoder.encoder.layer.8.attention.self.query.bias, text_encoder.encoder.layer.8.attention.self.key.weight, text_encoder.encoder.layer.8.attention.self.key.bias, text_encoder.encoder.layer.8.attention.self.value.weight, text_encoder.encoder.layer.8.attention.self.value.bias, text_encoder.encoder.layer.8.attention.output.dense.weight, text_encoder.encoder.layer.8.attention.output.dense.bias, text_encoder.encoder.layer.8.attention.output.LayerNorm.weight, text_encoder.encoder.layer.8.attention.output.LayerNorm.bias, text_encoder.encoder.layer.8.intermediate.dense.weight, text_encoder.encoder.layer.8.intermediate.dense.bias, text_encoder.encoder.layer.8.output.dense.weight, text_encoder.encoder.layer.8.output.dense.bias, text_encoder.encoder.layer.8.output.LayerNorm.weight, text_encoder.encoder.layer.8.output.LayerNorm.bias, text_encoder.encoder.layer.9.attention.self.query.weight, text_encoder.encoder.layer.9.attention.self.query.bias, text_encoder.encoder.layer.9.attention.self.key.weight, text_encoder.encoder.layer.9.attention.self.key.bias, text_encoder.encoder.layer.9.attention.self.value.weight, text_encoder.encoder.layer.9.attention.self.value.bias, text_encoder.encoder.layer.9.attention.output.dense.weight, text_encoder.encoder.layer.9.attention.output.dense.bias, text_encoder.encoder.layer.9.attention.output.LayerNorm.weight, text_encoder.encoder.layer.9.attention.output.LayerNorm.bias, text_encoder.encoder.layer.9.intermediate.dense.weight, text_encoder.encoder.layer.9.intermediate.dense.bias, text_encoder.encoder.layer.9.output.dense.weight, text_encoder.encoder.layer.9.output.dense.bias, text_encoder.encoder.layer.9.output.LayerNorm.weight, text_encoder.encoder.layer.9.output.LayerNorm.bias, text_encoder.encoder.layer.10.attention.self.query.weight, text_encoder.encoder.layer.10.attention.self.query.bias, text_encoder.encoder.layer.10.attention.self.key.weight, text_encoder.encoder.layer.10.attention.self.key.bias, text_encoder.encoder.layer.10.attention.self.value.weight, text_encoder.encoder.layer.10.attention.self.value.bias, text_encoder.encoder.layer.10.attention.output.dense.weight, text_encoder.encoder.layer.10.attention.output.dense.bias, text_encoder.encoder.layer.10.attention.output.LayerNorm.weight, text_encoder.encoder.layer.10.attention.output.LayerNorm.bias, text_encoder.encoder.layer.10.intermediate.dense.weight, text_encoder.encoder.layer.10.intermediate.dense.bias, text_encoder.encoder.layer.10.output.dense.weight, text_encoder.encoder.layer.10.output.dense.bias, text_encoder.encoder.layer.10.output.LayerNorm.weight, text_encoder.encoder.layer.10.output.LayerNorm.bias, text_encoder.encoder.layer.11.attention.self.query.weight, text_encoder.encoder.layer.11.attention.self.query.bias, text_encoder.encoder.layer.11.attention.self.key.weight, text_encoder.encoder.layer.11.attention.self.key.bias, text_encoder.encoder.layer.11.attention.self.value.weight, text_encoder.encoder.layer.11.attention.self.value.bias, text_encoder.encoder.layer.11.attention.output.dense.weight, text_encoder.encoder.layer.11.attention.output.dense.bias, text_encoder.encoder.layer.11.attention.output.LayerNorm.weight, text_encoder.encoder.layer.11.attention.output.LayerNorm.bias, text_encoder.encoder.layer.11.intermediate.dense.weight, text_encoder.encoder.layer.11.intermediate.dense.bias, text_encoder.encoder.layer.11.output.dense.weight, text_encoder.encoder.layer.11.output.dense.bias, text_encoder.encoder.layer.11.output.LayerNorm.weight, text_encoder.encoder.layer.11.output.LayerNorm.bias, text_encoder.pooler.dense.weight, text_encoder.pooler.dense.bias, dec.input_proj.0.weight, dec.input_proj.0.bias, dec.input_proj.1.weight, dec.input_proj.1.bias, dec.input_proj_2d.0.weight, dec.input_proj_2d.0.bias, dec.input_proj_2d.2.weight, dec.input_proj_2d.2.bias, dec.input_proj_2d.4.weight, dec.input_proj_2d.4.bias, dec.sum_norm.weight, dec.sum_norm.bias, dec.lang_proj.weight, dec.lang_proj.bias, dec.lang_norm.weight, dec.lang_norm.bias, dec.sampling_module.camap.query_proj.weight, dec.sampling_module.camap.key_proj.weight, dec.query_generator.0.weight, dec.query_generator.0.bias, dec.query_generator.2.weight, dec.query_generator.2.bias, dec.query_generator.4.weight, dec.query_generator.4.bias, dec.swa_layers.0.attn.in_proj_weight, dec.swa_layers.0.attn.in_proj_bias, dec.swa_layers.0.attn.out_proj.weight, dec.swa_layers.0.attn.out_proj.bias, dec.swa_layers.0.norm.weight, dec.swa_layers.0.norm.bias, dec.swa_layers.1.attn.in_proj_weight, dec.swa_layers.1.attn.in_proj_bias, dec.swa_layers.1.attn.out_proj.weight, dec.swa_layers.1.attn.out_proj.bias, dec.swa_layers.1.norm.weight, dec.swa_layers.1.norm.bias, dec.swa_layers.2.attn.in_proj_weight, dec.swa_layers.2.attn.in_proj_bias, dec.swa_layers.2.attn.out_proj.weight, dec.swa_layers.2.attn.out_proj.bias, dec.swa_layers.2.norm.weight, dec.swa_layers.2.norm.bias, dec.swa_layers.3.attn.in_proj_weight, dec.swa_layers.3.attn.in_proj_bias, dec.swa_layers.3.attn.out_proj.weight, dec.swa_layers.3.attn.out_proj.bias, dec.swa_layers.3.norm.weight, dec.swa_layers.3.norm.bias, dec.swa_layers.4.attn.in_proj_weight, dec.swa_layers.4.attn.in_proj_bias, dec.swa_layers.4.attn.out_proj.weight, dec.swa_layers.4.attn.out_proj.bias, dec.swa_layers.4.norm.weight, dec.swa_layers.4.norm.bias, dec.swa_layers.5.attn.in_proj_weight, dec.swa_layers.5.attn.in_proj_bias, dec.swa_layers.5.attn.out_proj.weight, dec.swa_layers.5.attn.out_proj.bias, dec.swa_layers.5.norm.weight, dec.swa_layers.5.norm.bias, dec.rra_layers.0.attn.in_proj_weight, dec.rra_layers.0.attn.in_proj_bias, dec.rra_layers.0.attn.out_proj.weight, dec.rra_layers.0.attn.out_proj.bias, dec.rra_layers.0.norm.weight, dec.rra_layers.0.norm.bias, dec.rra_layers.1.attn.in_proj_weight, dec.rra_layers.1.attn.in_proj_bias, dec.rra_layers.1.attn.out_proj.weight, dec.rra_layers.1.attn.out_proj.bias, dec.rra_layers.1.norm.weight, dec.rra_layers.1.norm.bias, dec.rra_layers.2.attn.in_proj_weight, dec.rra_layers.2.attn.in_proj_bias, dec.rra_layers.2.attn.out_proj.weight, dec.rra_layers.2.attn.out_proj.bias, dec.rra_layers.2.norm.weight, dec.rra_layers.2.norm.bias, dec.rra_layers.3.attn.in_proj_weight, dec.rra_layers.3.attn.in_proj_bias, dec.rra_layers.3.attn.out_proj.weight, dec.rra_layers.3.attn.out_proj.bias, dec.rra_layers.3.norm.weight, dec.rra_layers.3.norm.bias, dec.rra_layers.4.attn.in_proj_weight, dec.rra_layers.4.attn.in_proj_bias, dec.rra_layers.4.attn.out_proj.weight, dec.rra_layers.4.attn.out_proj.bias, dec.rra_layers.4.norm.weight, dec.rra_layers.4.norm.bias, dec.rra_layers.5.attn.in_proj_weight, dec.rra_layers.5.attn.in_proj_bias, dec.rra_layers.5.attn.out_proj.weight, dec.rra_layers.5.attn.out_proj.bias, dec.rra_layers.5.norm.weight, dec.rra_layers.5.norm.bias, dec.rla_layers.0.attn.in_proj_weight, dec.rla_layers.0.attn.in_proj_bias, dec.rla_layers.0.attn.out_proj.weight, dec.rla_layers.0.attn.out_proj.bias, dec.rla_layers.0.norm.weight, dec.rla_layers.0.norm.bias, dec.rla_layers.1.attn.in_proj_weight, dec.rla_layers.1.attn.in_proj_bias, dec.rla_layers.1.attn.out_proj.weight, dec.rla_layers.1.attn.out_proj.bias, dec.rla_layers.1.norm.weight, dec.rla_layers.1.norm.bias, dec.rla_layers.2.attn.in_proj_weight, dec.rla_layers.2.attn.in_proj_bias, dec.rla_layers.2.attn.out_proj.weight, dec.rla_layers.2.attn.out_proj.bias, dec.rla_layers.2.norm.weight, dec.rla_layers.2.norm.bias, dec.rla_layers.3.attn.in_proj_weight, dec.rla_layers.3.attn.in_proj_bias, dec.rla_layers.3.attn.out_proj.weight, dec.rla_layers.3.attn.out_proj.bias, dec.rla_layers.3.norm.weight, dec.rla_layers.3.norm.bias, dec.rla_layers.4.attn.in_proj_weight, dec.rla_layers.4.attn.in_proj_bias, dec.rla_layers.4.attn.out_proj.weight, dec.rla_layers.4.attn.out_proj.bias, dec.rla_layers.4.norm.weight, dec.rla_layers.4.norm.bias, dec.rla_layers.5.attn.in_proj_weight, dec.rla_layers.5.attn.in_proj_bias, dec.rla_layers.5.attn.out_proj.weight, dec.rla_layers.5.attn.out_proj.bias, dec.rla_layers.5.norm.weight, dec.rla_layers.5.norm.bias, dec.swa_ffn_layers.0.net.0.weight, dec.swa_ffn_layers.0.net.0.bias, dec.swa_ffn_layers.0.net.3.weight, dec.swa_ffn_layers.0.net.3.bias, dec.swa_ffn_layers.0.norm.weight, dec.swa_ffn_layers.0.norm.bias, dec.swa_ffn_layers.1.net.0.weight, dec.swa_ffn_layers.1.net.0.bias, dec.swa_ffn_layers.1.net.3.weight, dec.swa_ffn_layers.1.net.3.bias, dec.swa_ffn_layers.1.norm.weight, dec.swa_ffn_layers.1.norm.bias, dec.swa_ffn_layers.2.net.0.weight, dec.swa_ffn_layers.2.net.0.bias, dec.swa_ffn_layers.2.net.3.weight, dec.swa_ffn_layers.2.net.3.bias, dec.swa_ffn_layers.2.norm.weight, dec.swa_ffn_layers.2.norm.bias, dec.swa_ffn_layers.3.net.0.weight, dec.swa_ffn_layers.3.net.0.bias, dec.swa_ffn_layers.3.net.3.weight, dec.swa_ffn_layers.3.net.3.bias, dec.swa_ffn_layers.3.norm.weight, dec.swa_ffn_layers.3.norm.bias, dec.swa_ffn_layers.4.net.0.weight, dec.swa_ffn_layers.4.net.0.bias, dec.swa_ffn_layers.4.net.3.weight, dec.swa_ffn_layers.4.net.3.bias, dec.swa_ffn_layers.4.norm.weight, dec.swa_ffn_layers.4.norm.bias, dec.swa_ffn_layers.5.net.0.weight, dec.swa_ffn_layers.5.net.0.bias, dec.swa_ffn_layers.5.net.3.weight, dec.swa_ffn_layers.5.net.3.bias, dec.swa_ffn_layers.5.norm.weight, dec.swa_ffn_layers.5.norm.bias, dec.sem_cls_heads.0.net.0.weight, dec.sem_cls_heads.0.net.1.weight, dec.sem_cls_heads.0.net.1.bias, dec.sem_cls_heads.0.net.1.running_mean, dec.sem_cls_heads.0.net.1.running_var, dec.sem_cls_heads.0.net.4.weight, dec.sem_cls_heads.0.net.5.weight, dec.sem_cls_heads.0.net.5.bias, dec.sem_cls_heads.0.net.5.running_mean, dec.sem_cls_heads.0.net.5.running_var, dec.sem_cls_heads.0.net.8.weight, dec.sem_cls_heads.0.net.8.bias, dec.sem_cls_heads.1.net.0.weight, dec.sem_cls_heads.1.net.1.weight, dec.sem_cls_heads.1.net.1.bias, dec.sem_cls_heads.1.net.1.running_mean, dec.sem_cls_heads.1.net.1.running_var, dec.sem_cls_heads.1.net.4.weight, dec.sem_cls_heads.1.net.5.weight, dec.sem_cls_heads.1.net.5.bias, dec.sem_cls_heads.1.net.5.running_mean, dec.sem_cls_heads.1.net.5.running_var, dec.sem_cls_heads.1.net.8.weight, dec.sem_cls_heads.1.net.8.bias, dec.sem_cls_heads.2.net.0.weight, dec.sem_cls_heads.2.net.1.weight, dec.sem_cls_heads.2.net.1.bias, dec.sem_cls_heads.2.net.1.running_mean, dec.sem_cls_heads.2.net.1.running_var, dec.sem_cls_heads.2.net.4.weight, dec.sem_cls_heads.2.net.5.weight, dec.sem_cls_heads.2.net.5.bias, dec.sem_cls_heads.2.net.5.running_mean, dec.sem_cls_heads.2.net.5.running_var, dec.sem_cls_heads.2.net.8.weight, dec.sem_cls_heads.2.net.8.bias, dec.sem_cls_heads.3.net.0.weight, dec.sem_cls_heads.3.net.1.weight, dec.sem_cls_heads.3.net.1.bias, dec.sem_cls_heads.3.net.1.running_mean, dec.sem_cls_heads.3.net.1.running_var, dec.sem_cls_heads.3.net.4.weight, dec.sem_cls_heads.3.net.5.weight, dec.sem_cls_heads.3.net.5.bias, dec.sem_cls_heads.3.net.5.running_mean, dec.sem_cls_heads.3.net.5.running_var, dec.sem_cls_heads.3.net.8.weight, dec.sem_cls_heads.3.net.8.bias, dec.sem_cls_heads.4.net.0.weight, dec.sem_cls_heads.4.net.1.weight, dec.sem_cls_heads.4.net.1.bias, dec.sem_cls_heads.4.net.1.running_mean, dec.sem_cls_heads.4.net.1.running_var, dec.sem_cls_heads.4.net.4.weight, dec.sem_cls_heads.4.net.5.weight, dec.sem_cls_heads.4.net.5.bias, dec.sem_cls_heads.4.net.5.running_mean, dec.sem_cls_heads.4.net.5.running_var, dec.sem_cls_heads.4.net.8.weight, dec.sem_cls_heads.4.net.8.bias, dec.sem_cls_heads.5.net.0.weight, dec.sem_cls_heads.5.net.1.weight, dec.sem_cls_heads.5.net.1.bias, dec.sem_cls_heads.5.net.1.running_mean, dec.sem_cls_heads.5.net.1.running_var, dec.sem_cls_heads.5.net.4.weight, dec.sem_cls_heads.5.net.5.weight, dec.sem_cls_heads.5.net.5.bias, dec.sem_cls_heads.5.net.5.running_mean, dec.sem_cls_heads.5.net.5.running_var, dec.sem_cls_heads.5.net.8.weight, dec.sem_cls_heads.5.net.8.bias, dec.scg.0.attn.in_proj_weight, dec.scg.0.attn.in_proj_bias, dec.scg.0.attn.out_proj.weight, dec.scg.0.attn.out_proj.bias, dec.scg.0.norm.weight, dec.scg.0.norm.bias, dec.scg.1.attn.in_proj_weight, dec.scg.1.attn.in_proj_bias, dec.scg.1.attn.out_proj.weight, dec.scg.1.attn.out_proj.bias, dec.scg.1.norm.weight, dec.scg.1.norm.bias, dec.scg.2.attn.in_proj_weight, dec.scg.2.attn.in_proj_bias, dec.scg.2.attn.out_proj.weight, dec.scg.2.attn.out_proj.bias, dec.scg.2.norm.weight, dec.scg.2.norm.bias, dec.scg.3.attn.in_proj_weight, dec.scg.3.attn.in_proj_bias, dec.scg.3.attn.out_proj.weight, dec.scg.3.attn.out_proj.bias, dec.scg.3.norm.weight, dec.scg.3.norm.bias, dec.scg.4.attn.in_proj_weight, dec.scg.4.attn.in_proj_bias, dec.scg.4.attn.out_proj.weight, dec.scg.4.attn.out_proj.bias, dec.scg.4.norm.weight, dec.scg.4.norm.bias, dec.scg.5.attn.in_proj_weight, dec.scg.5.attn.in_proj_bias, dec.scg.5.attn.out_proj.weight, dec.scg.5.attn.out_proj.bias, dec.scg.5.norm.weight, dec.scg.5.norm.bias, dec.lqg.0.attn.in_proj_weight, dec.lqg.0.attn.in_proj_bias, dec.lqg.0.attn.out_proj.weight, dec.lqg.0.attn.out_proj.bias, dec.lqg.0.norm.weight, dec.lqg.0.norm.bias, dec.lqg.1.attn.in_proj_weight, dec.lqg.1.attn.in_proj_bias, dec.lqg.1.attn.out_proj.weight, dec.lqg.1.attn.out_proj.bias, dec.lqg.1.norm.weight, dec.lqg.1.norm.bias, dec.lqg.2.attn.in_proj_weight, dec.lqg.2.attn.in_proj_bias, dec.lqg.2.attn.out_proj.weight, dec.lqg.2.attn.out_proj.bias, dec.lqg.2.norm.weight, dec.lqg.2.norm.bias, dec.lqg.3.attn.in_proj_weight, dec.lqg.3.attn.in_proj_bias, dec.lqg.3.attn.out_proj.weight, dec.lqg.3.attn.out_proj.bias, dec.lqg.3.norm.weight, dec.lqg.3.norm.bias, dec.lqg.4.attn.in_proj_weight, dec.lqg.4.attn.in_proj_bias, dec.lqg.4.attn.out_proj.weight, dec.lqg.4.attn.out_proj.bias, dec.lqg.4.norm.weight, dec.lqg.4.norm.bias, dec.lqg.5.attn.in_proj_weight, dec.lqg.5.attn.in_proj_bias, dec.lqg.5.attn.out_proj.weight, dec.lqg.5.attn.out_proj.bias, dec.lqg.5.norm.weight, dec.lqg.5.norm.bias, dec.lla_layers.0.attn.in_proj_weight, dec.lla_layers.0.attn.in_proj_bias, dec.lla_layers.0.attn.out_proj.weight, dec.lla_layers.0.attn.out_proj.bias, dec.lla_layers.0.norm.weight, dec.lla_layers.0.norm.bias, dec.lla_layers.1.attn.in_proj_weight, dec.lla_layers.1.attn.in_proj_bias, dec.lla_layers.1.attn.out_proj.weight, dec.lla_layers.1.attn.out_proj.bias, dec.lla_layers.1.norm.weight, dec.lla_layers.1.norm.bias, dec.lla_layers.2.attn.in_proj_weight, dec.lla_layers.2.attn.in_proj_bias, dec.lla_layers.2.attn.out_proj.weight, dec.lla_layers.2.attn.out_proj.bias, dec.lla_layers.2.norm.weight, dec.lla_layers.2.norm.bias, dec.lla_layers.3.attn.in_proj_weight, dec.lla_layers.3.attn.in_proj_bias, dec.lla_layers.3.attn.out_proj.weight, dec.lla_layers.3.attn.out_proj.bias, dec.lla_layers.3.norm.weight, dec.lla_layers.3.norm.bias, dec.lla_layers.4.attn.in_proj_weight, dec.lla_layers.4.attn.in_proj_bias, dec.lla_layers.4.attn.out_proj.weight, dec.lla_layers.4.attn.out_proj.bias, dec.lla_layers.4.norm.weight, dec.lla_layers.4.norm.bias, dec.lla_layers.5.attn.in_proj_weight, dec.lla_layers.5.attn.in_proj_bias, dec.lla_layers.5.attn.out_proj.weight, dec.lla_layers.5.attn.out_proj.bias, dec.lla_layers.5.norm.weight, dec.lla_layers.5.norm.bias, dec.lsa_layers.0.attn.in_proj_weight, dec.lsa_layers.0.attn.in_proj_bias, dec.lsa_layers.0.attn.out_proj.weight, dec.lsa_layers.0.attn.out_proj.bias, dec.lsa_layers.0.norm.weight, dec.lsa_layers.0.norm.bias, dec.lsa_layers.1.attn.in_proj_weight, dec.lsa_layers.1.attn.in_proj_bias, dec.lsa_layers.1.attn.out_proj.weight, dec.lsa_layers.1.attn.out_proj.bias, dec.lsa_layers.1.norm.weight, dec.lsa_layers.1.norm.bias, dec.lsa_layers.2.attn.in_proj_weight, dec.lsa_layers.2.attn.in_proj_bias, dec.lsa_layers.2.attn.out_proj.weight, dec.lsa_layers.2.attn.out_proj.bias, dec.lsa_layers.2.norm.weight, dec.lsa_layers.2.norm.bias, dec.lsa_layers.3.attn.in_proj_weight, dec.lsa_layers.3.attn.in_proj_bias, dec.lsa_layers.3.attn.out_proj.weight, dec.lsa_layers.3.attn.out_proj.bias, dec.lsa_layers.3.norm.weight, dec.lsa_layers.3.norm.bias, dec.lsa_layers.4.attn.in_proj_weight, dec.lsa_layers.4.attn.in_proj_bias, dec.lsa_layers.4.attn.out_proj.weight, dec.lsa_layers.4.attn.out_proj.bias, dec.lsa_layers.4.norm.weight, dec.lsa_layers.4.norm.bias, dec.lsa_layers.5.attn.in_proj_weight, dec.lsa_layers.5.attn.in_proj_bias, dec.lsa_layers.5.attn.out_proj.weight, dec.lsa_layers.5.attn.out_proj.bias, dec.lsa_layers.5.norm.weight, dec.lsa_layers.5.norm.bias, dec.lsa_ffn_layers.0.net.0.weight, dec.lsa_ffn_layers.0.net.0.bias, dec.lsa_ffn_layers.0.net.3.weight, dec.lsa_ffn_layers.0.net.3.bias, dec.lsa_ffn_layers.0.norm.weight, dec.lsa_ffn_layers.0.norm.bias, dec.lsa_ffn_layers.1.net.0.weight, dec.lsa_ffn_layers.1.net.0.bias, dec.lsa_ffn_layers.1.net.3.weight, dec.lsa_ffn_layers.1.net.3.bias, dec.lsa_ffn_layers.1.norm.weight, dec.lsa_ffn_layers.1.norm.bias, dec.lsa_ffn_layers.2.net.0.weight, dec.lsa_ffn_layers.2.net.0.bias, dec.lsa_ffn_layers.2.net.3.weight, dec.lsa_ffn_layers.2.net.3.bias, dec.lsa_ffn_layers.2.norm.weight, dec.lsa_ffn_layers.2.norm.bias, dec.lsa_ffn_layers.3.net.0.weight, dec.lsa_ffn_layers.3.net.0.bias, dec.lsa_ffn_layers.3.net.3.weight, dec.lsa_ffn_layers.3.net.3.bias, dec.lsa_ffn_layers.3.norm.weight, dec.lsa_ffn_layers.3.norm.bias, dec.lsa_ffn_layers.4.net.0.weight, dec.lsa_ffn_layers.4.net.0.bias, dec.lsa_ffn_layers.4.net.3.weight, dec.lsa_ffn_layers.4.net.3.bias, dec.lsa_ffn_layers.4.norm.weight, dec.lsa_ffn_layers.4.norm.bias, dec.lsa_ffn_layers.5.net.0.weight, dec.lsa_ffn_layers.5.net.0.bias, dec.lsa_ffn_layers.5.net.3.weight, dec.lsa_ffn_layers.5.net.3.bias, dec.lsa_ffn_layers.5.norm.weight, dec.lsa_ffn_layers.5.norm.bias, dec.sem_cls_head.net.0.weight, dec.sem_cls_head.net.1.weight, dec.sem_cls_head.net.1.bias, dec.sem_cls_head.net.1.running_mean, dec.sem_cls_head.net.1.running_var, dec.sem_cls_head.net.4.weight, dec.sem_cls_head.net.5.weight, dec.sem_cls_head.net.5.bias, dec.sem_cls_head.net.5.running_mean, dec.sem_cls_head.net.5.running_var, dec.sem_cls_head.net.8.weight, dec.sem_cls_head.net.8.bias, dec.out_norm.weight, dec.out_norm.bias, dec.out_score.0.weight, dec.out_score.0.bias, dec.out_score.2.weight, dec.out_score.2.bias, dec.x_mask.0.weight, dec.x_mask.0.bias, dec.x_mask.2.weight, dec.x_mask.2.bias, dec.indi_embedding.0.weight, dec.indi_embedding.0.bias, dec.indi_embedding.2.weight, dec.indi_embedding.2.bias, dec.indi_embedding.3.weight, dec.indi_embedding.3.bias, dec.indi_norm.weight, dec.indi_norm.bias, dec.contrastive_align_projection_vision.0.weight, dec.contrastive_align_projection_vision.0.bias, dec.contrastive_align_projection_vision.2.weight, dec.contrastive_align_projection_vision.2.bias, dec.contrastive_align_projection_vision.4.weight, dec.contrastive_align_projection_vision.4.bias, dec.contrastive_align_projection_text.0.weight, dec.contrastive_align_projection_text.0.bias, dec.contrastive_align_projection_text.2.weight, dec.contrastive_align_projection_text.2.bias, dec.contrastive_align_projection_text.4.weight, dec.contrastive_align_projection_text.4.bias, dec.scg_head.attn.in_proj_weight, dec.scg_head.attn.in_proj_bias, dec.scg_head.attn.out_proj.weight, dec.scg_head.attn.out_proj.bias, dec.scg_head.norm.weight, dec.scg_head.norm.bias, dec.scg_head.glu_p.weight, dec.scg_head.glu_p.bias, dec.scg_head.glu_g.weight, dec.scg_head.glu_g.bias, scene_encoder.superpoint_proj.0.weight, scene_encoder.superpoint_proj.0.bias, scene_encoder.superpoint_proj.1.weight, scene_encoder.superpoint_proj.1.bias, scene_encoder.superpoint_proj.4.weight, scene_encoder.superpoint_proj.4.bias, scene_encoder.superpoint_proj.5.weight, scene_encoder.superpoint_proj.5.bias, scene_encoder.attn_pool.0.weight, scene_encoder.attn_pool.0.bias, scene_encoder_criterion.pos_weight

2025-06-13 11:28:23,047 - INFO - Train batch size per gpu: 2
2025-06-13 11:28:23,279 - INFO - Load train multi3drefer: 43838 samples
2025-06-13 11:28:27,036 - INFO - Load val multi3drefer: 11120 samples
2025-06-13 11:28:28,106 - INFO - Training
2025-06-13 11:29:05,788 - INFO - Epoch [1/70][10/2860]  lr: 0.0001, eta: 8 days, 17:29:34, data_time: 0.35, iter_time: 3.77, score_loss: 0.0000, mask_bce_loss: 0.0333, mask_dice_loss: 0.1489, sem_loss: 5.2007, indi_loss: 2.5762, sample_loss: 0.1446, layer_0_score_loss: 0.0000, layer_0_mask_bce_loss: 0.0516, layer_0_mask_dice_loss: 0.1439, layer_0_sem_loss: 5.3350, layer_0_indi_loss: 2.7416, layer_1_score_loss: 0.0000, layer_1_mask_bce_loss: 0.0298, layer_1_mask_dice_loss: 0.1431, layer_1_sem_loss: 5.2031, layer_1_indi_loss: 2.5933, layer_2_score_loss: 0.0000, layer_2_mask_bce_loss: 0.0404, layer_2_mask_dice_loss: 0.1478, layer_2_sem_loss: 5.3391, layer_2_indi_loss: 2.5662, layer_3_score_loss: 0.0000, layer_3_mask_bce_loss: 0.0433, layer_3_mask_dice_loss: 0.1460, layer_3_sem_loss: 5.1896, layer_3_indi_loss: 2.5404, layer_4_score_loss: 0.0000, layer_4_mask_bce_loss: 0.0406, layer_4_mask_dice_loss: 0.1451, layer_4_sem_loss: 5.1065, layer_4_indi_loss: 2.5299, layer_5_score_loss: 0.0000, layer_5_mask_bce_loss: 0.0362, layer_5_mask_dice_loss: 0.1485, layer_5_sem_loss: 5.1204, layer_5_indi_loss: 2.5650, loss: 5.6127, scene_loss: 1.0497, total_loss: 5.6127, grad_total_norm: 4.4003
2025-06-13 11:29:17,325 - INFO - Epoch [1/70][20/2860]  lr: 0.0001, eta: 5 days, 16:49:20, data_time: 0.18, iter_time: 2.46, score_loss: 0.0000, mask_bce_loss: 0.0271, mask_dice_loss: 0.1473, sem_loss: 4.7663, indi_loss: 2.3326, sample_loss: 0.1457, layer_0_score_loss: 0.0000, layer_0_mask_bce_loss: 0.0397, layer_0_mask_dice_loss: 0.1450, layer_0_sem_loss: 4.8500, layer_0_indi_loss: 2.5829, layer_1_score_loss: 0.0000, layer_1_mask_bce_loss: 0.0346, layer_1_mask_dice_loss: 0.1454, layer_1_sem_loss: 4.7104, layer_1_indi_loss: 2.3578, layer_2_score_loss: 0.0000, layer_2_mask_bce_loss: 0.0264, layer_2_mask_dice_loss: 0.1456, layer_2_sem_loss: 4.8148, layer_2_indi_loss: 2.3172, layer_3_score_loss: 0.0000, layer_3_mask_bce_loss: 0.0285, layer_3_mask_dice_loss: 0.1476, layer_3_sem_loss: 4.9231, layer_3_indi_loss: 2.3112, layer_4_score_loss: 0.0000, layer_4_mask_bce_loss: 0.0301, layer_4_mask_dice_loss: 0.1469, layer_4_sem_loss: 4.7909, layer_4_indi_loss: 2.2939, layer_5_score_loss: 0.0000, layer_5_mask_bce_loss: 0.0241, layer_5_mask_dice_loss: 0.1472, layer_5_sem_loss: 4.6739, layer_5_indi_loss: 2.3191, loss: 5.4818, scene_loss: 1.2110, total_loss: 5.4818, grad_total_norm: 3.7732
2025-06-13 11:29:27,803 - INFO - Epoch [1/70][30/2860]  lr: 0.0001, eta: 4 days, 14:37:41, data_time: 0.12, iter_time: 1.99, score_loss: 0.0000, mask_bce_loss: 0.0214, mask_dice_loss: 0.0607, sem_loss: 3.8047, indi_loss: 1.4805, sample_loss: 0.1494, layer_0_score_loss: 0.0000, layer_0_mask_bce_loss: 0.0184, layer_0_mask_dice_loss: 0.0582, layer_0_sem_loss: 3.7246, layer_0_indi_loss: 1.5955, layer_1_score_loss: 0.0000, layer_1_mask_bce_loss: 0.0255, layer_1_mask_dice_loss: 0.0614, layer_1_sem_loss: 3.3916, layer_1_indi_loss: 1.4928, layer_2_score_loss: 0.0000, layer_2_mask_bce_loss: 0.0260, layer_2_mask_dice_loss: 0.0620, layer_2_sem_loss: 3.6049, layer_2_indi_loss: 1.4790, layer_3_score_loss: 0.0000, layer_3_mask_bce_loss: 0.0192, layer_3_mask_dice_loss: 0.0587, layer_3_sem_loss: 3.7294, layer_3_indi_loss: 1.4746, layer_4_score_loss: 0.0000, layer_4_mask_bce_loss: 0.0200, layer_4_mask_dice_loss: 0.0590, layer_4_sem_loss: 3.5216, layer_4_indi_loss: 1.4744, layer_5_score_loss: 0.0000, layer_5_mask_bce_loss: 0.0233, layer_5_mask_dice_loss: 0.0614, layer_5_sem_loss: 3.7606, layer_5_indi_loss: 1.4808, loss: 3.9813, scene_loss: 0.8397, total_loss: 3.9813, grad_total_norm: 10.2323
2025-06-13 11:29:39,009 - INFO - Epoch [1/70][40/2860]  lr: 0.0001, eta: 4 days, 2:32:36, data_time: 0.09, iter_time: 1.77, score_loss: 0.0000, mask_bce_loss: 0.0180, mask_dice_loss: 0.0471, sem_loss: 2.6492, indi_loss: 1.1835, sample_loss: 0.1395, layer_0_score_loss: 0.0000, layer_0_mask_bce_loss: 0.0135, layer_0_mask_dice_loss: 0.0445, layer_0_sem_loss: 2.4383, layer_0_indi_loss: 1.1896, layer_1_score_loss: 0.0000, layer_1_mask_bce_loss: 0.0227, layer_1_mask_dice_loss: 0.0483, layer_1_sem_loss: 2.3136, layer_1_indi_loss: 1.1717, layer_2_score_loss: 0.0000, layer_2_mask_bce_loss: 0.0257, layer_2_mask_dice_loss: 0.0484, layer_2_sem_loss: 2.2250, layer_2_indi_loss: 1.1779, layer_3_score_loss: 0.0000, layer_3_mask_bce_loss: 0.0277, layer_3_mask_dice_loss: 0.0487, layer_3_sem_loss: 2.4960, layer_3_indi_loss: 1.1820, layer_4_score_loss: 0.0000, layer_4_mask_bce_loss: 0.0211, layer_4_mask_dice_loss: 0.0475, layer_4_sem_loss: 2.2977, layer_4_indi_loss: 1.1841, layer_5_score_loss: 0.0000, layer_5_mask_bce_loss: 0.0212, layer_5_mask_dice_loss: 0.0480, layer_5_sem_loss: 2.4304, layer_5_indi_loss: 1.1836, loss: 3.1779, scene_loss: 0.7539, total_loss: 3.1779, grad_total_norm: 6.5382
2025-06-13 11:29:51,387 - INFO - Epoch [1/70][50/2860]  lr: 0.0001, eta: 3 days, 20:35:35, data_time: 0.07, iter_time: 1.67, score_loss: 0.0000, mask_bce_loss: 0.0135, mask_dice_loss: 0.2679, sem_loss: 6.5346, indi_loss: 2.9936, sample_loss: 0.1448, layer_0_score_loss: 0.0000, layer_0_mask_bce_loss: 0.0160, layer_0_mask_dice_loss: 0.2649, layer_0_sem_loss: 6.2660, layer_0_indi_loss: 4.1136, layer_1_score_loss: 0.0000, layer_1_mask_bce_loss: 0.0216, layer_1_mask_dice_loss: 0.2698, layer_1_sem_loss: 6.1078, layer_1_indi_loss: 3.2186, layer_2_score_loss: 0.0000, layer_2_mask_bce_loss: 0.0210, layer_2_mask_dice_loss: 0.2696, layer_2_sem_loss: 6.2296, layer_2_indi_loss: 3.0899, layer_3_score_loss: 0.0000, layer_3_mask_bce_loss: 0.0151, layer_3_mask_dice_loss: 0.2685, layer_3_sem_loss: 6.2990, layer_3_indi_loss: 3.0378, layer_4_score_loss: 0.0000, layer_4_mask_bce_loss: 0.0140, layer_4_mask_dice_loss: 0.2675, layer_4_sem_loss: 6.4006, layer_4_indi_loss: 3.0066, layer_5_score_loss: 0.0000, layer_5_mask_bce_loss: 0.0139, layer_5_mask_dice_loss: 0.2684, layer_5_sem_loss: 6.5163, layer_5_indi_loss: 2.9989, loss: 6.6115, scene_loss: 0.9651, total_loss: 6.6115, grad_total_norm: 8.5142
2025-06-13 11:30:05,212 - INFO - Epoch [1/70][60/2860]  lr: 0.0001, eta: 3 days, 17:58:09, data_time: 0.06, iter_time: 1.62, score_loss: 0.0000, mask_bce_loss: 0.0249, mask_dice_loss: 0.2385, sem_loss: 6.3985, indi_loss: 2.7359, sample_loss: 0.1463, layer_0_score_loss: 0.0000, layer_0_mask_bce_loss: 0.0198, layer_0_mask_dice_loss: 0.2198, layer_0_sem_loss: 6.2892, layer_0_indi_loss: 3.9269, layer_1_score_loss: 0.0000, layer_1_mask_bce_loss: 0.0194, layer_1_mask_dice_loss: 0.2331, layer_1_sem_loss: 6.0995, layer_1_indi_loss: 2.9602, layer_2_score_loss: 0.0000, layer_2_mask_bce_loss: 0.0188, layer_2_mask_dice_loss: 0.2336, layer_2_sem_loss: 6.1459, layer_2_indi_loss: 2.8271, layer_3_score_loss: 0.0000, layer_3_mask_bce_loss: 0.0312, layer_3_mask_dice_loss: 0.2371, layer_3_sem_loss: 6.3748, layer_3_indi_loss: 2.7800, layer_4_score_loss: 0.0000, layer_4_mask_bce_loss: 0.0244, layer_4_mask_dice_loss: 0.2389, layer_4_sem_loss: 6.3092, layer_4_indi_loss: 2.7462, layer_5_score_loss: 0.0000, layer_5_mask_bce_loss: 0.0242, layer_5_mask_dice_loss: 0.2371, layer_5_sem_loss: 6.2024, layer_5_indi_loss: 2.7417, loss: 6.5567, scene_loss: 1.1426, total_loss: 6.5567, grad_total_norm: 5.9246
2025-06-13 11:30:16,968 - INFO - Epoch [1/70][70/2860]  lr: 0.0001, eta: 3 days, 14:26:53, data_time: 0.05, iter_time: 1.56, score_loss: 0.0000, mask_bce_loss: 0.0194, mask_dice_loss: 0.0758, sem_loss: 3.5070, indi_loss: 1.5712, sample_loss: 0.1440, layer_0_score_loss: 0.0002, layer_0_mask_bce_loss: 0.0151, layer_0_mask_dice_loss: 0.0661, layer_0_sem_loss: 3.4427, layer_0_indi_loss: 1.7803, layer_1_score_loss: 0.0000, layer_1_mask_bce_loss: 0.0186, layer_1_mask_dice_loss: 0.0739, layer_1_sem_loss: 3.3225, layer_1_indi_loss: 1.5662, layer_2_score_loss: 0.0000, layer_2_mask_bce_loss: 0.0206, layer_2_mask_dice_loss: 0.0771, layer_2_sem_loss: 3.4565, layer_2_indi_loss: 1.5628, layer_3_score_loss: 0.0000, layer_3_mask_bce_loss: 0.0248, layer_3_mask_dice_loss: 0.0775, layer_3_sem_loss: 3.3732, layer_3_indi_loss: 1.5664, layer_4_score_loss: 0.0000, layer_4_mask_bce_loss: 0.0212, layer_4_mask_dice_loss: 0.0768, layer_4_sem_loss: 3.3824, layer_4_indi_loss: 1.5698, layer_5_score_loss: 0.0000, layer_5_mask_bce_loss: 0.0200, layer_5_mask_dice_loss: 0.0764, layer_5_sem_loss: 3.4336, layer_5_indi_loss: 1.5708, loss: 3.8732, scene_loss: 0.7648, total_loss: 3.8732, grad_total_norm: 5.2096
2025-06-13 11:30:29,381 - INFO - Epoch [1/70][80/2860]  lr: 0.0001, eta: 3 days, 12:15:47, data_time: 0.04, iter_time: 1.52, score_loss: 0.0000, mask_bce_loss: 0.0179, mask_dice_loss: 0.0961, sem_loss: 3.3316, indi_loss: 1.5794, sample_loss: 0.1415, layer_0_score_loss: 0.0000, layer_0_mask_bce_loss: 0.0111, layer_0_mask_dice_loss: 0.0868, layer_0_sem_loss: 3.1794, layer_0_indi_loss: 1.7776, layer_1_score_loss: 0.0000, layer_1_mask_bce_loss: 0.0160, layer_1_mask_dice_loss: 0.0955, layer_1_sem_loss: 3.1984, layer_1_indi_loss: 1.5640, layer_2_score_loss: 0.0000, layer_2_mask_bce_loss: 0.0151, layer_2_mask_dice_loss: 0.0930, layer_2_sem_loss: 3.0665, layer_2_indi_loss: 1.5688, layer_3_score_loss: 0.0000, layer_3_mask_bce_loss: 0.0159, layer_3_mask_dice_loss: 0.0935, layer_3_sem_loss: 3.1236, layer_3_indi_loss: 1.5701, layer_4_score_loss: 0.0000, layer_4_mask_bce_loss: 0.0154, layer_4_mask_dice_loss: 0.0940, layer_4_sem_loss: 3.2184, layer_4_indi_loss: 1.5773, layer_5_score_loss: 0.0000, layer_5_mask_bce_loss: 0.0162, layer_5_mask_dice_loss: 0.0955, layer_5_sem_loss: 2.9905, layer_5_indi_loss: 1.5725, loss: 3.9211, scene_loss: 0.8723, total_loss: 3.9211, grad_total_norm: 5.8608
2025-06-13 11:30:41,335 - INFO - Epoch [1/70][90/2860]  lr: 0.0001, eta: 3 days, 10:16:51, data_time: 0.04, iter_time: 1.48, score_loss: 0.0000, mask_bce_loss: 0.0225, mask_dice_loss: 0.1004, sem_loss: 3.4226, indi_loss: 1.6703, sample_loss: 0.1440, layer_0_score_loss: 0.0001, layer_0_mask_bce_loss: 0.0155, layer_0_mask_dice_loss: 0.0901, layer_0_sem_loss: 3.3044, layer_0_indi_loss: 1.9812, layer_1_score_loss: 0.0000, layer_1_mask_bce_loss: 0.0181, layer_1_mask_dice_loss: 0.0993, layer_1_sem_loss: 3.3133, layer_1_indi_loss: 1.6666, layer_2_score_loss: 0.0000, layer_2_mask_bce_loss: 0.0187, layer_2_mask_dice_loss: 0.0993, layer_2_sem_loss: 3.4314, layer_2_indi_loss: 1.6624, layer_3_score_loss: 0.0000, layer_3_mask_bce_loss: 0.0216, layer_3_mask_dice_loss: 0.1026, layer_3_sem_loss: 3.6047, layer_3_indi_loss: 1.6602, layer_4_score_loss: 0.0000, layer_4_mask_bce_loss: 0.0193, layer_4_mask_dice_loss: 0.1013, layer_4_sem_loss: 3.3396, layer_4_indi_loss: 1.6656, layer_5_score_loss: 0.0000, layer_5_mask_bce_loss: 0.0202, layer_5_mask_dice_loss: 0.1007, layer_5_sem_loss: 3.4590, layer_5_indi_loss: 1.6667, loss: 4.4445, scene_loss: 1.1985, total_loss: 4.4445, grad_total_norm: 7.9346
slurmstepd: error: *** JOB 53471 ON todi.disi.unitn.it CANCELLED AT 2025-06-13T11:30:52 ***
2025-06-13 11:30:52,462 - INFO - Epoch [1/70][100/2860]  lr: 0.0001, eta: 3 days, 8:13:59, data_time: 0.04, iter_time: 1.44, score_loss: 0.0000, mask_bce_loss: 0.0115, mask_dice_loss: 0.0740, sem_loss: 2.2168, indi_loss: 1.4175, sample_loss: 0.1428, layer_0_score_loss: 0.0004, layer_0_mask_bce_loss: 0.0090, layer_0_mask_dice_loss: 0.0635, layer_0_sem_loss: 2.0904, layer_0_indi_loss: 1.3949, layer_1_score_loss: 0.0001, layer_1_mask_bce_loss: 0.0105, layer_1_mask_dice_loss: 0.0691, layer_1_sem_loss: 2.0451, layer_1_indi_loss: 1.3593, layer_2_score_loss: 0.0001, layer_2_mask_bce_loss: 0.0106, layer_2_mask_dice_loss: 0.0691, layer_2_sem_loss: 2.2124, layer_2_indi_loss: 1.3807, layer_3_score_loss: 0.0000, layer_3_mask_bce_loss: 0.0132, layer_3_mask_dice_loss: 0.0704, layer_3_sem_loss: 2.0736, layer_3_indi_loss: 1.3943, layer_4_score_loss: 0.0000, layer_4_mask_bce_loss: 0.0125, layer_4_mask_dice_loss: 0.0727, layer_4_sem_loss: 1.9291, layer_4_indi_loss: 1.4042, layer_5_score_loss: 0.0000, layer_5_mask_bce_loss: 0.0113, layer_5_mask_dice_loss: 0.0734, layer_5_sem_loss: 2.1053, layer_5_indi_loss: 1.4053, loss: 3.3726, scene_loss: 0.9230, total_loss: 3.3726, grad_total_norm: 6.1342
2025-06-13 11:31:03,168 - INFO - Epoch [1/70][110/2860]  lr: 0.0001, eta: 3 days, 6:20:44, data_time: 0.03, iter_time: 1.41, score_loss: 0.0000, mask_bce_loss: 0.0067, mask_dice_loss: 0.0943, sem_loss: 3.3852, indi_loss: 1.6235, sample_loss: 0.1405, layer_0_score_loss: 0.0000, layer_0_mask_bce_loss: 0.0067, layer_0_mask_dice_loss: 0.0846, layer_0_sem_loss: 3.3759, layer_0_indi_loss: 1.8553, layer_1_score_loss: 0.0012, layer_1_mask_bce_loss: 0.0059, layer_1_mask_dice_loss: 0.0898, layer_1_sem_loss: 3.2380, layer_1_indi_loss: 1.6049, layer_2_score_loss: 0.0000, layer_2_mask_bce_loss: 0.0062, layer_2_mask_dice_loss: 0.0941, layer_2_sem_loss: 3.0470, layer_2_indi_loss: 1.6048, layer_3_score_loss: 0.0000, layer_3_mask_bce_loss: 0.0062, layer_3_mask_dice_loss: 0.0934, layer_3_sem_loss: 3.1243, layer_3_indi_loss: 1.6087, layer_4_score_loss: 0.0000, layer_4_mask_bce_loss: 0.0062, layer_4_mask_dice_loss: 0.0940, layer_4_sem_loss: 2.9832, layer_4_indi_loss: 1.6190, layer_5_score_loss: 0.0000, layer_5_mask_bce_loss: 0.0059, layer_5_mask_dice_loss: 0.0940, layer_5_sem_loss: 3.4200, layer_5_indi_loss: 1.6188, loss: 4.2195, scene_loss: 1.1740, total_loss: 4.2195, grad_total_norm: 10.7144
2025-06-13 11:31:14,562 - INFO - Epoch [1/70][120/2860]  lr: 0.0001, eta: 3 days, 5:05:23, data_time: 0.03, iter_time: 1.39, score_loss: 0.0000, mask_bce_loss: 0.0091, mask_dice_loss: 0.0786, sem_loss: 2.7521, indi_loss: 1.5252, sample_loss: 0.1388, layer_0_score_loss: 0.0003, layer_0_mask_bce_loss: 0.0074, layer_0_mask_dice_loss: 0.0653, layer_0_sem_loss: 2.6237, layer_0_indi_loss: 1.6290, layer_1_score_loss: 0.0000, layer_1_mask_bce_loss: 0.0100, layer_1_mask_dice_loss: 0.0744, layer_1_sem_loss: 2.7049, layer_1_indi_loss: 1.4941, layer_2_score_loss: 0.0000, layer_2_mask_bce_loss: 0.0103, layer_2_mask_dice_loss: 0.0755, layer_2_sem_loss: 2.6998, layer_2_indi_loss: 1.5062, layer_3_score_loss: 0.0000, layer_3_mask_bce_loss: 0.0089, layer_3_mask_dice_loss: 0.0791, layer_3_sem_loss: 2.7666, layer_3_indi_loss: 1.5089, layer_4_score_loss: 0.0000, layer_4_mask_bce_loss: 0.0091, layer_4_mask_dice_loss: 0.0787, layer_4_sem_loss: 2.7371, layer_4_indi_loss: 1.5151, layer_5_score_loss: 0.0000, layer_5_mask_bce_loss: 0.0093, layer_5_mask_dice_loss: 0.0792, layer_5_sem_loss: 2.6889, layer_5_indi_loss: 1.5144, loss: 3.6349, scene_loss: 0.8961, total_loss: 3.6349, grad_total_norm: 4.7538
